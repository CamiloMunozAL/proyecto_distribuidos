# ğŸ§ª GuÃ­a de DemostraciÃ³n â€“ Replica Sets MongoDB (db1, db2, db3)

> ğŸ¯ Objetivo: Mostrar alque la base de datos distribuida funciona con **replicaciÃ³n**, **sharding** y **failover automÃ¡tico** entre contenedores Incus.

---

## ğŸ” 1ï¸âƒ£ Ver contenedores activos e IPs

```bash
incus list
```

âœ… Espera ver algo como:
```
| NAME   | STATE   | IPV4             |
|--------|----------|-----------------|
| db1    | RUNNING | 10.122.112.153  |
| db2    | RUNNING | 10.122.112.233  |
| db3    | RUNNING | 10.122.112.16   |
```

---

## ğŸ’¾ 2ï¸âƒ£ Ver estado de los Replica Sets

### ğŸ”¹ Shard A (rs_products_a)
```bash
incus exec db1 -- mongosh --port 27017 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

âœ… Resultado esperado:
```
db1:27017 PRIMARY
db2:27018 SECONDARY
db3:27018 ARBITER
```

---

### ğŸ”¹ Shard B (rs_products_b)
```bash
incus exec db2 -- mongosh --port 27017 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

âœ… Resultado esperado:
```
db2:27017 PRIMARY
db1:27018 SECONDARY
db3:27019 ARBITER
```

---

### ğŸ”¹ Replica Set de Usuarios (rs_users)
```bash
incus exec db3 -- mongosh --port 27017 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

âœ… Resultado esperado:
```
db1:27019 PRIMARY
db3:27017 SECONDARY
```

ğŸ—£ï¸ **ExplicaciÃ³n:**  
Cada rÃ©plica tiene un **PRIMARY** (lÃ­der), un **SECONDARY** (copia) y un **ARBITER** (votante).  
Los cambios se replican automÃ¡ticamente y el sistema puede recuperarse solo si un nodo cae.

---

## ğŸ“¤ 3ï¸âƒ£ Probar replicaciÃ³n en Shard A

### â• Insertar un documento en el PRIMARY
```bash
incus exec db1 -- mongosh --port 27017 --eval 'use products_db; db.products.insertOne({nombre:"Manzana", precio:100, shard:"A"})'
```

### ğŸ” Verificar que se replicÃ³ en el SECONDARY
```bash
incus exec db2 -- mongosh --port 27018 --eval 'rs.secondaryOk(); use products_db; db.products.find({nombre:"Manzana"}).pretty()'
```

âœ… Si aparece el documento, la replicaciÃ³n funciona correctamente.

---

## ğŸ”„ 4ï¸âƒ£ Probar failover automÃ¡tico

### ğŸ“´ Simular caÃ­da del PRIMARY (db1)
```bash
incus stop db1
```

### ğŸ” Ver nuevo PRIMARY (esperar ~10 segundos)
```bash
incus exec db2 -- mongosh --port 27018 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

âœ… DeberÃ­as ver:
```
db2:27018 PRIMARY
db3:27018 ARBITER
```

ğŸ—£ï¸ **Explica:**  
â€œMongoDB eligiÃ³ automÃ¡ticamente un nuevo PRIMARY con ayuda del Ã¡rbitro en `db3`.â€

---

### âš™ï¸ Recuperar el nodo caÃ­do
```bash
incus start db1
sleep 10
incus exec db1 -- mongosh --port 27017 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

âœ… `db1` vuelve como **SECONDARY** y se sincroniza.

---

## ğŸ“¦ 5ï¸âƒ£ Probar replicaciÃ³n en Shard B

### â• Insertar un producto (Shard Nâ€“Z)
```bash
incus exec db2 -- mongosh --port 27017 --eval 'use products_db; db.products.insertOne({nombre:"Zanahoria", precio:80, shard:"B"})'
```

### ğŸ” Verificar en el SECONDARY
```bash
incus exec db1 -- mongosh --port 27018 --eval 'rs.secondaryOk(); use products_db; db.products.find({nombre:"Zanahoria"}).pretty()'
```

âœ… Si ves el documento, la replicaciÃ³n del Shard B tambiÃ©n estÃ¡ activa.

---

## ğŸ‘¥ 6ï¸âƒ£ Verificar el Replica Set de Usuarios

```bash
incus exec db3 -- mongosh --port 27017 --eval 'rs.status().members.forEach(m => print(m.name, m.stateStr))'
```

ğŸ—£ï¸ â€œAquÃ­ se guardan los usuarios del sistema (auth).  
Si uno de los nodos cae, el otro toma el rol de PRIMARY automÃ¡ticamente.â€

---

## ğŸ§¹ 7ï¸âƒ£ Limpiar (Eliminar productos de prueba)

Para dejar la base lista y repetir la demostraciÃ³n:

### Eliminar documentos en Shard A
```bash
incus exec db1 -- mongosh --port 27017 --eval 'use products_db; db.products.deleteMany({nombre:{$in:["Manzana"]}})'
```

### Eliminar documentos en Shard B
```bash
incus exec db2 -- mongosh --port 27017 --eval 'use products_db; db.products.deleteMany({nombre:{$in:["Zanahoria"]}})'
```

âœ… Ahora puedes volver a insertar productos y repetir la demo sin duplicados.

---

## âœ… 8ï¸âƒ£ ConclusiÃ³n para la presentaciÃ³n

> â€œAquÃ­ demuestro que mis tres contenedores de base de datos (`db1`, `db2`, `db3`) funcionan de forma coordinada:  
> - Cada shard tiene su propio PRIMARY, SECONDARY y ARBITER.  
> - Los datos se replican automÃ¡ticamente entre nodos.  
> - Si un nodo se apaga, otro toma el liderazgo sin perder datos.  
> - Puedo limpiar los registros y repetir la prueba en cualquier momento.â€

---

ğŸ“˜ **Fin de la demostraciÃ³n**
